\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\renewcommand*{\proofname}{Proof}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\theoremstyle{example}
\newtheorem{example}{Example}[section]
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{fullpage}
%\usepackage{svg}
\usepackage{pgf}
\allowdisplaybreaks

\newcommand{\AAA}{\mathbf{A}}
\newcommand{\uu}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\betat}{\hat{\beta}}
\newcommand{\mx}{\overline{x}}
\newcommand{\my}{\overline{y}}
\newcommand{\mxy}{\overline{x}\overline{y}}
\newcommand{\mxsquare}{\overline{x^2}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\x}{\mathbf{x}}

\newcommand{\lambdafunction}[2][1]{
		\let\tmp\relax
		\newcommand{\tmp}[#1]{#2}
		\tmp
}


\newcommand{\ffmatrix}[3]{
		\begin{bmatrix}
				\lambdafunction[2]{#1}{1}{1} & \lambdafunction[2]{#1}{1}{2} & \cdots & \lambdafunction[2]{#1}{1}{#3}\\
				\lambdafunction[2]{#1}{2}{1} & \lambdafunction[2]{#1}{2}{2} & \cdots & \lambdafunction[2]{#1}{2}{#3}\\
				\vdots & \vdots & \ddots & \vdots\\
				\lambdafunction[2]{#1}{#2}{1} & \lambdafunction[2]{#1}{#2}{2} & \cdots & \lambdafunction[2]{#1}{#2}{#3}\\
		\end{bmatrix}
}

\newcommand{\fmatrix}[3]{\begin{bmatrix} #1_{1 1} & #1_{1 2} & \cdots & #1_{1 #3} \\ #1_{2 1} & #1_{2 2} & \cdots & #1_{2 #3} \\ \vdots & \vdots & \ddots & \vdots \\ #1_{#2 1} & #1_{#2 2} & \cdots & #1_{#2 #3} \end{bmatrix}}
\newcommand{\fvector}[2]{\begin{bmatrix} #1_1 \\ #1_2 \\ \vdots \\ #1_{#2} \end{bmatrix}}

\title{Regression Analysis}
\author{Lysandre Terrisse}

\begin{document}
\maketitle

\section{Introduction}

\begin{definition}[Regression analysis]
		Let $\mathcal{X}$ be an \textit{independent variable} over $\mathbb{R}^n$, or equivalently a set of $n$ independent variables $\{\mathcal{X}_1, \dots, \mathcal{X}_n\}$ over $\mathbb{R}$. It may for instance be a single independent variable representing the amount of $CO_2$ in the atmosphere in parts per million (ppm). Let $\mathcal{Y}$ be a \textit{dependent} variable over $\mathbb{R}$. For instance, it may be the dependent variable representing the global warming relative to the 1850-1900 period. We would want to determine the relationship between $\mathcal{X}$ and $\mathcal{Y}$, that is, we want to find $(\mathcal{Y} \mid \mathcal{X})$. To do so, we do $N$ measurements of $\mathcal{Y}$ given some values for $\mathcal{X}$, and therefore obtain $N$ points $(x_1, y_1), \dots, (x_N, y_N)$. However, as there may be errors during the measurement of $\mathcal{Y}$, the samples aren't distributed according to $(\mathcal{Y} \mid \mathcal{X})$, but according to $(\mathcal{Y} + \epsilon \mid \mathcal{X})$, where $\epsilon$ is a random variable that represents the unknown measurement error. The reason why the measurement error is unknown is that, if we were to know it, we could just adjust our data to correct this error. To visualize our data, we can plot it like so:

\begin{figure}[h!]
		\centering
		\scalebox{0.9}{\input{images/climate_change.pgf}}
\end{figure}

		Now, to find $(\mathcal{Y} \mid \mathcal{X})$, we may try to find a curve that passes well through the data. For instance, we may try to use the function $f(x, \beta) = \beta_0 + \beta_1 x$ with parameters $\beta_0 = -2.94$ and $\beta_1 = 0.01$. In that case, we get:

\begin{figure}[h!]
		\centering
		\scalebox{0.9}{\input{images/OLS_climate_change.pgf}}
\end{figure}

		This is pretty good! But, how have we determined that curve? And are there better ones? These are the questions that the field of \textit{regression analysis} tries to answer, which is crucial in order to study many real-life phenomena. But, is the fact that the curve fits the points almost perfectly mean that $(\mathcal{Y} \mid \mathcal{X}) = f(\mathcal{X}, \beta)$ with parameters $\beta_0 = -2.94$ and $\beta_1 = 0.01$? Not necessarily. Indeed, maybe the fact that the points are almost all aligned is just a measurement problem. Or maybe the function has this shape for values around this interval, but then increases exponentially for higher values. In other words, we cannot find $(\mathcal{Y} \mid \mathcal{X})$ from the $K$ observations without more information about $(\mathcal{Y} \mid \mathcal{X})$ and $\epsilon$. In regression analysis problems, we will therefore be given a function $f : \mathbb{R}^n \times \mathbb{R}^k$, called the \textit{regression model}, such that there exists a parameter $\beta \in \mathbb{R}^k$ such that $(\mathcal{Y} \mid \mathcal{X}) = f(\mathcal{X}, \beta)$. Our problem now just consists of finding $\beta$. However, we still don't have enough information to be $100\%$ confident about the real value of $\beta$.
\end{definition}

\begin{definition}[Regression problem]
		A regression problem is a tuple $(n, k, N, f, \beta, \epsilon, X, Y)$ such that:
		\begin{itemize}
				\item $n \in \mathbb{N}^*$ is the number of independent variables
				\item $k \in \mathbb{N}^*$ is the number of parameters
				\item $N \in \mathbb{N}^*$ is the sample size (i.e. the number of observations we have made).
				\item $f : \mathbb{R}^n \times \mathbb{R}^k \rightarrow \mathbb{R}$ and $\beta \in \mathbb{R}^k$ are respectively the regression model and the parameter (which is as a column vector) for which $f(\mathcal{X}, \beta) = (\mathcal{Y} \mid \mathcal{X})$
				\item $\epsilon \in \mathbb{R}^N$ is a column vector where $\epsilon_i$ is the measurement error when measuring $\mathcal{Y}$ on the $i$-th observation
				\item $X$ is a $N \times n$ matrix where $x_{ij}$ is the value of $\mathcal{X}_j$ on the $i$-th observation
				\item $Y \in \mathbb{R}^N$ is a column vector where $y_i = f(x_i, \beta) + \epsilon_i$ is the error-prone measure of $\mathcal{Y}$ on the $i$-th observation.
		\end{itemize}
		The goal of the regression problem is, from $f$, $X$, and $Y$, to find $\beta$.
\end{definition}

\begin{remark}
		To verify whether a tuple $(n, k, N, f, \beta, \epsilon, X, Y)$ is a valid regression problem, we just need to verify that $n$, $k$, $N$, $f$, $\epsilon$, $X$, and $Y$ have the right type (e.g. that $f$ has the type $\mathbb{R}^n \times \mathbb{R}^k \rightarrow \mathbb{R}$), and that for all $i \in \{1, \dots, N\}, y_i = f(x_i, \beta) + \epsilon_i$.
\end{remark}

\section{Linear regression}

\begin{definition}[Linear regression problems]
		The regression problem $(n, k=n+1, N, f, \beta, \epsilon, X, Y)$ is said to be \textit{linear} when:

		\begin{equation} \label{eqn1}
				\forall \betat \in \mathbb{R}^k, \forall \vv \in \mathbb{R}^n, f(\vv, \betat) = \begin{bmatrix} 1 \\ \vv_1 \\ \vdots \\ \vv_n \end{bmatrix}^\top \begin{bmatrix} \betat_0 \\ \betat_1 \\ \vdots \\ \betat_n \end{bmatrix} = \betat_0 + \betat_1 \vv_1 + \dots + \betat_n \vv_n
		\end{equation}
		Which implies that:
				$$\forall \betat \in \mathbb{R}^k, \forall i \in \{1, \dots, N\}, f(x_i, \betat) = \begin{bmatrix} 1 \\ x_{i1} \\ \vdots \\ x_{in} \end{bmatrix}^\top \begin{bmatrix} \betat_0 \\ \betat_1 \\ \vdots \\ \betat_n \end{bmatrix} = \betat_0 + \betat_1 x_{i1} + \dots + \betat_n x_{in}$$
		Or equivalently:
				$$\forall \betat \in \mathbb{R}^k, \forall i \in \{1, \dots, n\}, y_i = \begin{bmatrix} 1 \\ x_{i1} \\ \vdots \\ x_{in} \end{bmatrix}^\top \begin{bmatrix} \betat_0 \\ \betat_1 \\ \vdots \\ \betat_n \end{bmatrix} + \epsilon_i$$
		Or equivalently:
				$$\forall \betat \in \mathbb{R}^k, \fvector{y}{N} = \begin{bmatrix} 1 & x_{11} & \cdots & x_{1n} \\ 1 & x_{21} & \cdots & x_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & x_{N1} & \cdots & x_{Nn} \end{bmatrix} \begin{bmatrix} \betat_0 \\ \betat_1 \\ \vdots \\ \betat_n \end{bmatrix} + \fvector{\epsilon}{N} = X' \betat + \epsilon$$
		where $X'$ is the matrix $X$ for which we add a column of 1 to the left. When the regression problem is linear, we say that $f$ is a \textit{linear model}. Note also that the first element of the parameters $\betat$ is not $\betat_1$ but $\betat_0$.
\end{definition}

\begin{remark}
		In linear algebra, linear functions are functions between two vector spaces which respect additivity (i.e. $f(u + v) = f(u) + f(v)$) and homogeneity (i.e. $f(\lambda u) = \lambda f(u)$). If the two vector spaces are finite-dimensional (with $a$ and $b$ dimensions respectively), then linear functions are exactly the functions of the form $f(x) = Ax$, where $A$ is a $b \times a$ matrix. We see from the definition of linear models that they are linear according to $\beta$ (because the impact of $f$ on $\beta$ can be represented by the matrix $X'$).
\end{remark}

\begin{remark}
		To verify whether a tuple $(n, k, N, f, \beta, \epsilon, X, Y)$ is a valid linear regression poblem, we apply the same method than in the last remark, and we verify that equation (\ref{eqn1}) holds.
\end{remark}

\subsection{Simple linear regression}

\begin{definition}
		A linear regression problem $(n, k, N, f, \eta, \epsilon, X, Y)$ is said to be \textit{simple} when $n=1$. That is, the dependent variable $\mathcal{Y}$ depends only of a single variable $\mathcal{X}_1$. Since $n=1$, then $k=2$ (as $k=n+1$ for linear regression problems), and therefore the regression model $f$ is:
				$$f : \mathbb{R} \times \mathbb{R}^2 \rightarrow \mathbb{R}$$
				$$\vv, \betat \mapsto \betat_0 + \betat_1 \vv_1$$
		Simple linear regression problems can be plotted in a 2-dimensional plane (given any $\betat$), just as we did in the introduction, and the graph of $f$ will be a straight line.
\end{definition}

We have already seen that, since we have made only finitely many observations, we cannot deduce with certainty the exact value of $\beta$. So, one intuitive goal is to find $\betat$ that fits \textit{best} the observations. This is the idea of the next technique, which is the most widely used method for linear regression.

\begin{definition}[Ordinary Least Squares]
		In simple linear regression, when we have at least two points of distinct $x$ value, \textit{Ordinary Least Squares} (OLS) consists of choosing the value $\beta$ whose straight line minimizes the squared differences between the observations and that line. That is, it will choose:
				$$\betat = \argmin_{\betat \in \mathbb{R}^2} RSS(\betat) = \argmin_{\betat \in \mathbb{R}^2} \sum_{i=1}^N (y_i - f(x_i, \betat))^2 = \argmin_{\betat \in \mathbb{R}^2} \sum_{i=1}^N (y_i - (\betat_0 + \betat_1 x_i))^2$$
		Where $RSS(\betat) = \sum_{i=1}^N (y_i - f(x_i, \betat))^2$ is called the \textit{residual sum of squares}.
\end{definition}

We have already seen an example of what ordinary least squares looks like when applied on some real-world data. Indeed, in the introduction, the values of $\beta$ were chosen (and rounded up to two digits of precision) by the ordinary least squares. To better understand what ordinary least squares does on this example, let's draw in red (vertical lines) the distances $(y_i - f(x_i, \betat))$:

\begin{figure}[h!]
		\centering
		\scalebox{0.7}{\input{images/OLS_climate_change_with_residuals.pgf}}
\end{figure}

To visualize what the residual sum of squares is, just imagine squaring each such red line, and then summing these squared values. This is what ordinary least squares tries to minimize. Notice that, in an intuitive sense, when the straight line doesn't fit well with the observations, the residual sum of squares is high, whereas if the line were to fit perfectly with all points, the residual sum of squares would be zero. We will now see a way to solve ordinary least squares for simple linear regression problems.

\begin{theorem} %https://statproofbook.github.io/P/slr-ols.html
		For simple linear regression models, the \textit{unique} values for $\betat$ that will be chosen by ordinary least squares are:
				$$\betat_1 = \frac{\sum_{i=1}^N (x_i - \overline{x})(y_i - \overline{y})}{\sum_{i=1}^N (x_i - \overline{x})^2}$$
				$$\betat_0 = \overline{y} - \betat_1 \overline{x}$$
		With $\overline{x} = \frac{1}{N}\sum_{i=1}^N x_i$ and $\overline{y} = \frac{1}{n}\sum_{i=1}^N y_i$ are respectively the mean of the $x_i$ and the mean of the $y_i$.
\end{theorem}

\begin{proof}
		The derivatives of $RSS$ with respect to $\betat_0$ and $\betat_1$ are
		\begin{align*}
				\frac{\partial RSS}{\partial \betat_0} &= \frac{\partial \sum_{i=1}^N (y_i - (\betat_0 + \betat_1 x_i))^2}{\partial \betat_0}\\
															   &= \frac{\partial \sum_{i=1}^N ((-\betat_0) + (y_i - \betat_1 x_i))^2}{\partial \betat_0}\\
															   &= \frac{\partial \sum_{i=1}^N (\betat_0^2 + 2 (-\betat_0) (y_i - \betat_1 x_i) + (y_i - \betat_1 x_i)^2)}{\partial \betat_0}\\
															   &= \sum_{i=1}^N (2\betat_0 + 2(\betat_1 x_i - y_i))\\
															   &= \sum_{i=1}^N 2(\betat_0 + \betat_1 x_i - y_i)\\
															   &=  2(N \betat_0 + \betat_1 (\sum_{i=1}^N x_i) - (\sum_{i=1}^N y_i))\\
															   &=  2(N \betat_0 + N \betat_1 \mx - N \my)\\
															   &=  2N(\betat_0 + \betat_1 \mx - \my)\\
		\end{align*}
		\begin{align*}
				\frac{\partial RSS}{\partial \betat_1} &= \frac{\partial \sum_{i=1}^N (y_i - (\betat_0 + \betat_1 x_i))^2}{\partial \betat_1}\\
															   &= \frac{\partial \sum_{i=1}^N ((-\betat_1 x_i) + (y_i - \betat_0))^2}{\partial \betat_1}\\
															   &= \frac{\partial \sum_{i=1}^N ((-\betat_1 x_i)^2 + 2(-\betat_1 x_i)(y_i - \betat_0) + (y_i - \betat_0)^2)}{\partial \betat_1}\\
															   &= \sum_{i=1}^N (2\betat_1 x_i^2 + 2x_i(\betat_0 - y_i))\\
															   &= \sum_{i=1}^N 2(\betat_0 x_i + \betat_1 x_i^2 - x_i y_i)\\
															   &= 2(\betat_0 (\sum_{i=1}^N x_i) + \betat_1 (\sum_{i=1}^N x_i^2) - (\sum_{i=1}^N x_i y_i))\\
															   &= 2(N \betat_0 \mx + N \betat_1 \mxsquare - N \mxy)\\
															   &= 2N(\betat_0 \mx + \betat_1 \mxsquare - \mxy)
		\end{align*}

		Now, let $\betat = \argmin_{\betat \in \mathbb{R}^2} RSS(\betat)$. As $RSS$ reaches its minimum when given $\betat$, these derivatives are equal to zero when evaluated on $\betat$:
		\begin{align*}
				&2N(\betat_0 + \betat_1 \mx - \my) = 0\\
				&\iff \betat_0 + \betat_1 \mx - \my = 0\\
				&\iff \betat_0 = \my - \betat_1 \mx\\
		\end{align*}
		\begin{align*}
				&2N(\betat_0 \mx + \betat_1 \mxsquare - \mxy) = 0\\
				&\iff \betat_0 \mx + \betat_1 \mxsquare - \mxy = 0\\
				&\iff (\my - \betat_1 \mx) \mx + \betat_1 \mxsquare - \mxy = 0\\
				&\iff \mx\,\my - \betat_1 \mx^2 + \betat_1 \mxsquare - \mxy = 0\\
				&\iff \betat_1 (\mxsquare - \mx^2) + \mx\,\my - \mxy = 0\\
				&\iff \betat_1 = \frac{\mxy - \mx\,\my}{\mxsquare - \mx^2}
		\end{align*}
		By rewriting the numerator and the denominator independently, $\beta_1$ can be rewritten as:
		\begin{align*}
				\betat_1 &= \frac{\mxy - \mx\,\my}{\mxsquare - \mx^2}\\
						 &= \frac{\mxy - \mx\,\my + \mx\,\my - \mx\,\my}{\mxsquare - 2\mx^2 + \mx^2}\\
						 &= \frac{\frac{1}{N} (\sum_{i=1}^N x_i y_i) - \frac{1}{N} (\sum_{i=1}^N x_i \my) + \frac{1}{N} (\sum_{i=1}^N \mx y_i) - \frac{1}{N} (\sum_{i=1}^n \mx\,\my)}{\frac{1}{N}(\sum_{i=1}^N x_i^2) - 2\mx\frac{1}{N}(\sum_{i=1}^N x_i) + \frac{1}{N}(\sum_{i=1}^n \mx^2)}\\
						 &= \frac{\frac{1}{N} \sum_{i=1}^N (x_i y_i - x_i \my + \mx y_i - \mx\,\my)}{\frac{1}{N}(\sum_{i=1}^N x_i^2) - \frac{1}{N} (\sum_{i=1}^N 2 x_i \mx) + \frac{1}{N}(\sum_{i=1}^n \mx^2)}\\
						 &= \frac{\frac{1}{N} \sum_{i=1}^N (x_i - \mx)(y_i - \my)}{\frac{1}{N} \sum_{i=1}^N (x_i^2 - 2 x_i \mx + \mx^2)}\\
						 &= \frac{\sum_{i=1}^N (x_i - \mx)(y_i - \my)}{\sum_{i=1}^N (x_i - \mx)^2}
		\end{align*}
		Which concludes the proof. Note that, as we have assumed we had made at least two observations of distinct $x$ value, then the denominator has to be strictly positive.
\end{proof}

%\begin{remark}
%	Why use the squared distances, instead of the distances?
%\end{remark}

\subsection{Multiple linear regression}

\begin{definition}
		A \textit{multiple} linear regression problem is a linear regression problem $(n, k=n+1, N, f, \beta, \epsilon, X, Y)$ with $n>1$. In that case, the regression model $f$ is:
				$$f : \mathbb{R}^n \times \mathbb{R}^{n+1} \rightarrow \mathbb{R}$$
				$$\vv, \betat \mapsto \betat_0 + \betat_1 \vv_1 + \dots + \betat_n \vv_n$$ 
		which can be plotted on a $n+1$-dimensional space for any given $\betat$. The graph of $f$ will be an $n$-dimensional hyperplane. For instance, when $n=2$, we will have a 3-dimensional space containing points, and $f$ will be a 2-dimensional plane.
\end{definition}

Just as in single linear regression, ordinary least squares consists of finding the $\betat$ which minimizes the residual sum of squares. We will now see a way to solve multiple linear regression problems. However, we firstly need a few definitions.

\begin{definition}
		Let $f : \mathbb{R}^n \rightarrow \mathbb{R}$ be a function from vectors to reals. Suppose that $f$ admits a partial derivative $\frac{\partial f}{\vv_i}$ for all $i \in \{1, \dots, n\}$ (or $i \in \{0, \dots, n-1\}$ if we start the indexing at 0). In that case, we can define the gradient $\nabla f : \mathbb{R}^n \rightarrow \mathbb{R}^n$ as:
		$$\nabla f = \x \mapsto \begin{bmatrix} \frac{\partial f}{\partial \vv_1}(\x) \\ \frac{\partial f}{\partial \vv_2}(\x) \\ \dots \\ \frac{\partial f}{\partial \vv_n}(\x) \end{bmatrix}$$
		Or equivalently:
				$$(\nabla f)(\x) = \begin{bmatrix} \frac{\partial f}{\partial \vv_1}(\x) \\ \frac{\partial f}{\partial \vv_2}(\x) \\ \dots \\ \frac{\partial f}{\partial \vv_n}(\x) \end{bmatrix} \in \mathbb{R}^n$$
\end{definition}

The gradient has very interesting properties. As it is a generalization of the derivative, some formulas which hold for derivatives also hold for the gradient. We will now prove some of these properties, which are resumed in the next table. For the table of derivatives, we have $u, v : \mathbb{R} \rightarrow \mathbb{R}$, whereas for the table of gradients, we have $u, v : \mathbb{R}^n \rightarrow \mathbb{R}$. Some rows in the table of derivatives do not have an equivalent row for the table of gradients. For instance, the row containing the function $x \mapsto v(u(x))$ doesn't have an equivalent row for gradients. This is because, in this case, we would have a type error, as $v$ is supposed to take an element of $\mathbb{R}^n$ whereas it is given an element of $\mathbb{R}$. To give another example, the row containing the function $x \mapsto \cos(x)$ does not have an equivalent, because the cosinus of a vector (even when defined as the element wise cosinus of this vector) does not return a real, and therefore the function $f$ would return a vector instead of a real.

\begin{center} \small
\begin{tabular}{ |c|c| }
		\hline
		$f : \mathbb{R} \rightarrow \mathbb{R}$ & $f' : \mathbb{R} \rightarrow \mathbb{R}$\\
		\hline
		$x \mapsto u(x) + v(x)$ & $x \mapsto u'(x) + v'(x)$\\
		\hline
		$x \mapsto ku(x), k \in \mathbb{R}$ & $x \mapsto ku'(x)$\\
		\hline
		$x \mapsto u(x)v(x)$ & $x \mapsto u'(x)v(x) + u(x)v'(x)$\\
		\hline
		$x \mapsto (u(x))^n, n \in \mathbb{Z}^*$ & $x \mapsto nu'(x)(u(x))^{n-1}$\\
		\hline
		$x \mapsto \sqrt{u(x)}$ & $x \mapsto \frac{u'(x)}{2\sqrt{u(x)}}$\\
		\hline
		$x \mapsto \frac{1}{u(x)}$ & $x \mapsto -\frac{u'(x)}{(u(x))^2}$\\
		\hline
		$x \mapsto \frac{u(x)}{v(x)}$ & $x \mapsto \frac{u'(x)v(x) - u(x)v'(x)}{(v(x))^2}$\\
		\hline
		$x \mapsto e^{u(x)}$ & $x \mapsto u'(x)e^{u(x)}$\\
		\hline
		$x \mapsto \ln u(x)$ & $x \mapsto \frac{u'(x)}{u(x)}$\\
		\hline
		$x \mapsto k, k \in \mathbb{R}$ & $x \mapsto 0$\\
		\hline
		$x \mapsto v(u(x))$ & $x \mapsto v'(u(x)) u'(x)$\\
		\hline
		$x \mapsto x^n, n \in \mathbb{N}^*$ & $x \mapsto nx^{n-1}$\\
		\hline
		$x \mapsto \frac{1}{x}$ & $x \mapsto -\frac{1}{x^2}$\\
		\hline
		$x \mapsto \frac{1}{x^n}, n \in \mathbb{N}^*$ & $x \mapsto -\frac{n}{x^{n+1}}$\\
		\hline
		$x \mapsto \sqrt{x}$ & $x \mapsto \frac{1}{2\sqrt{x}}$\\
		\hline
		$x \mapsto e^{ax + b}, a, b \in \mathbb{R}$ & $ae^{ax + b}$\\
		\hline
		$x \mapsto e^x$ & $x \mapsto e^x$\\
		\hline
		$x \mapsto \ln x$ & $x \mapsto \frac{1}{x}$\\
		\hline
		$x \mapsto \cos(x)$ & $x \mapsto -\sin(x)$\\
		\hline
		$x \mapsto \arccos(x)$ & $x \mapsto -\frac{1}{\sqrt{1 - x^2}}$\\
		\hline
		$x \mapsto \sin(x)$ & $x \mapsto \cos(x)$\\
		\hline
		$x \mapsto \arcsin(x)$ & $x \mapsto \frac{1}{\sqrt{1 - x^2}}$\\
		\hline
		$x \mapsto \tan(x)$ & $x \mapsto \frac{1}{(\cos(x))^2}$\\
		\hline
		$x \mapsto \arctan(x)$ & $x \mapsto \frac{1}{1 + x^2}$\\
		\hline
\end{tabular}
\quad
\begin{tabular}{ |c|c| }
		\hline
		$f : \mathbb{R}^n \rightarrow \mathbb{R}$ & $\nabla f : \mathbb{R}^n \rightarrow \mathbb{R}^n$\\
		\hline
		$\x \mapsto u(\x) + v(\x)$ & $\x \mapsto (\nabla u)(\x) + (\nabla v)(\x)$\\
		\hline
		$\x \mapsto ku(\x), k \in \mathbb{R}$ & $\x \mapsto k(\nabla u)(\x)$\\
		\hline
		$\x \mapsto u(\x)v(\x)$ & $\x \mapsto (\nabla u)(\x)v(\x) + u(\x)(\nabla v)(\x)$\\
		\hline
		$\x \mapsto (u(\x))^n, n \in \mathbb{Z}^*$ & $\x \mapsto n(\nabla u)(\x)(u(\x))^{n-1}$\\
		\hline
		$\x \mapsto \sqrt{u(\x)}$ & $x \mapsto \frac{(\nabla u)(x)}{2\sqrt{u(x)}}$\\
		\hline
		$\x \mapsto \frac{1}{u(\x)}$ & $\x \mapsto -\frac{(\nabla u)(\x)}{(u(\x))^2}$\\
		\hline
		$\x \mapsto \frac{u(\x)}{v(\x)}$ & $\x \mapsto \frac{(\nabla u)(\x)v(\x) - u(\x)(\nabla v)(\x)}{(v(\x))^2}$\\
		\hline
		$\x \mapsto e^{u(\x)}$ & $\x \mapsto (\nabla u)(\x)e^{u(\x)}$\\
		\hline
		$\x \mapsto \ln u(\x)$ & $x \mapsto \frac{(\nabla u)(\x)}{u(x)}$\\
		\hline
		$\x \mapsto k, k \in \mathbb{R}$ & $x \mapsto 0$ \text{(where 0 is the zero vector)}\\
		\hline
\end{tabular}
\end{center}

Understanding the similarity between the derivative in one dimension and the gradient in many dimensions is very important for the next proof. Another similarity is that, when a differentiable function $f$ reaches its minimum for one value $\x$, then the gradient evaluated at this value is $(\nabla f)(\x) = 0$.

\begin{theorem}[Normal equation]
		For multiple (and single) linear regression models, the values of $\betat$ that can be chosen by ordinary least squares respect this equation, called the \textit{normal equation}:\textit{unique} value for $\betat$ that will be chosen by ordinary least squares is:
				$$X'^\top X' \betat = X'^\top y$$
		where $X'$ is the matrix $X$ for which we add a column of 1 to the left.
\end{theorem}

\begin{proof} %https://statproofbook.github.io/P/mlr-ols2
		Let's firstly remember some properties of the transpose operator. If $\AAA$ is a matrix and $\uu$ and $\vv$ are column vectors, then we have:
				$$(\AAA\vv)^\top = \vv^\top \AAA^\top$$
				$$(\uu + \vv)^\top = \uu^\top + \vv^\top$$
				$$\uu \AAA \vv = \vv^\top \AAA^\top \uu^\top$$

		We then have:
		\begin{align*}
				RSS(\betat) &= \sum_{i=1}^N (y_i - f(x_i, \betat))^2\\
							&= \sum_{i=1}^N (y_i - x'_i \betat)^2\\
							&= (y - X' \betat)^\top (y - X' \betat)\\
							&= (y^\top - (X' \betat)^\top)(y - X' \betat)\\
							&= (y^\top - \betat^\top X'^\top)(y - X' \betat)\\
							&= y^\top y - y^\top X' \betat - \betat^\top X'^\top y + \betat^\top X'^\top X' \betat \text{ (we distribute)}\\
							&= y^\top y - \betat^\top X'^\top y - \betat^\top X'^\top y + \betat^\top X'^\top X' \betat \text{ (we apply the third rule)}\\
							&= y^\top y - 2 \betat^\top X'^\top y + \betat^\top X'^\top X' \betat
		\end{align*}
		The gradient of $RSS$ is: %TODO Mieux expliquer le gradient
				$$(\nabla RSS)(\betat) = -2X'^\top y + 2X'^\top X' \betat$$
		Now, let $\betat = \argmin_{\betat \in \mathbb{R}^n} RSS(\betat)$ be the value of $\betat$ chosen by the ordinary least squares. As $\betat$ is the minimum of the function $RSS$, then the gradient of $RSS$ is null when evaluated on $\betat$:
		\begin{align*}
				&-2X'^\top y + 2X'^\top X' \betat = 0\\
				&\iff -X'^\top y + X'^\top X' \betat = 0\\
				&\iff X'^\top X' \betat = X'^\top y
		\end{align*}
\end{proof}

%web.cs.ucla/~chohsieh/teaching/CS260_Winter2019/notes_linearregression.pdf

If $X'^\top X$ is invertible, then the normal equation admits a \textit{unique} trivial solution:
		$$\betat = (X'^\top X')^{-1}X'^\top y$$
However, when $X^\top X$ isn't invertible, then 



\section{Polynomial regression}

\begin{definition}
		The regression problem $(n=1, k, N, f, \beta, \epsilon, X, Y)$ is said to be \textit{polynomial} when:
				$$\forall \betat \in \mathbb{R}^k, \forall x \in \mathbb{R}, f(x, \betat) = \betat_0 + \betat_1 x + \betat_2 x^2 + \dots + \betat_{k-1} x^{k-1}$$
		Which implies that:
		$$\forall \betat \in \mathbb{R}^k, \fvector{y}{N} = \begin{bmatrix} 1 & x_1 & x_1^2 & \cdots & x_1^{k-1} \\ 1 & x_2 & x_2^2 & \cdots & x_2^{k-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_N & x_N^2 & \cdots & x_N^{k-1} \end{bmatrix} \begin{bmatrix} \betat_0 \\ \betat_1 \\ \vdots \\ \betat_{k-1} \end{bmatrix} + \fvector{\epsilon}{N}$$
\end{definition}




%A curve fitting a line
%A picture fitting a class
%An AI fitting an objective

\section{Linear reduction}

\begin{definition}
		A regression problem $RP = (n, k, N, f, \beta, \epsilon, Y)$ is reducible to another regression problem $RP' = (n', k', N', g, \beta', \epsilon', Y')$ when we can convert the solution $\beta'$ of $RP'$ into the solution $\beta$ of $RP$. A regression problem $RP$ is said to be \textit{linearizable} when we can reduce $RP$ to a linear regression problem $RP'$. %,  is linearizable when it can \textit{easily} be transformed into a linear regression problem $RP' = (n', k, N', f', \beta, \epsilon', Y')$ which has the same solution $\beta$. This concept is informal, as there is no precise definition of what we mean by "easy".
\end{definition}

\begin{example}[Regression problems with a strictly positive exponential curve without errors can be linearized]
		Suppose we try to solve a regression problem $RP = (n=1, k=2, N, f, \beta, \epsilon, X, Y)$, where there is no measurement error (i.e. $\epsilon$ is null), and where the graph of $f$ is a strictly positive (i.e. $\beta_0 > 0$) exponential curve:
				$$f : \mathbb{R} \times \mathbb{R}^2 \rightarrow \mathbb{R}$$
				$$\vv, \hat{\beta} \mapsto \hat{\beta}_0 e^{\hat{\beta}_1 \vv_1}$$
		This regrssion problem isn't linear. To reduce the non-linear regression problem $RP$ to a linear regression prolem $RP'$, instead of studying the relationship between $\mathcal{X}$ and $\mathcal{Y}$, we will try to study the relationship between $\mathcal{X}$ and $\ln \mathcal{Y} $. That is, we will linearize our data by creating a column vector $Y'$ containing the logarithmic values of $Y$. As $\epsilon_i = 0$, and as $\beta_0 > 0$, we have $y_i = f(x_i, \beta) + \epsilon_i = f(x_i, \beta) = \beta_0 e^{\hat{\beta_1} x_{i1}} > 0$, and so $\ln y_i$ is defined. Then, we define $Y'$ like so:
				$$Y' = \ln Y = \fvector{\ln y}{N}$$
		And we define $\beta'$ like so:
				$$\beta' = \begin{bmatrix} \beta'_0 \\ \beta'_1 \end{bmatrix} = \begin{bmatrix} \ln \beta_0 \\ \beta_1 \end{bmatrix}$$
		And let's also define $g$ as:
				$$g : \mathbb{R} \times \mathbb{R}^2 \rightarrow \mathbb{R}$$
				$$\vv, \hat{\beta} \mapsto \hat{\beta}_0 + \hat{\beta}_1 \vv_1$$
		Then, $RP' = (n=1, k=2, N, g, \beta', \epsilon, X, Y')$ is a regression problem, as we have:
		\begin{align*}
				y' &= \ln y_i\\
				   &= \ln (f(x_i, \beta) + \epsilon_i)\\
				   &= \ln f(x_i, \beta)\\
				   &= \ln \beta_0 e^{\beta_1 x_{i1}}\\
				   &= (\ln \beta_0) + (\ln e^{\beta_1 x_{i1}})\\
				   &= \beta'_0 + \beta'_1 x_{i1}\\
				   &= g(x_i, \beta')\\
				   &= g(x_i, \beta') + \epsilon_i
		\end{align*}
		Furthermore, $RP'$ is linear, which we can see by looking at the definition of $g$. Finally, we can find $\beta$ from $\beta'$ like so:
				$$\beta = \begin{bmatrix} e^{\beta'_0} \\ \beta'_1 \end{bmatrix}$$
		And therefore, we have reduced the non-linear problem $RP$ to the linear problem $RP'$.
\end{example}

\begin{remark}
		We may wonder, would the previous technique work by assuming the weaker assumption $y_i > 0$ instead of $\epsilon_i = 0$? The answer is no. In fact, we wouldn't end up with a regression problem, since when trying to linearize by taking the logarithm of the predicted value, the measurement error becomes multiplicative instead of additive.
\end{remark}

\begin{example}[Regression problems with a parabola can be linearized]
		Suppose we try to solve a regression problem $RP = (n=1, k=3, N, f, \beta, \epsilon, X, Y)$, where $f$ is a parabola:
				$$f : \mathbb{R} \times \mathbb{R}^3 \rightarrow \mathbb{R}$$
				$$\vv, \beta \mapsto \beta_0 + \beta_1 \vv_1 + \beta_2 \vv_1^2$$
		In this form, $f$ isn't a linear model for $RP$. We will now see how to go from the non-linear regression problem $RP$ to an equivalent linear regression problem $RP'$. To do so, we will change $X$ (which, since $n=1$, has a single column) by adding one column that contains the squared value of the pre-existing column. That is, we define $X'$ as:
				$$X' = \begin{bmatrix} x'_{11} & x'_{12} \\ x'_{21} & x'_{22} \\ \vdots & \vdots \\ x'_{N1} & x'_{N2} \end{bmatrix} \begin{bmatrix} x_{11} & x_{11}^2 \\ x_{21} & x_{21}^2 \\ \vdots & \vdots \\ x_{N1} & x_{N1}^2 \end{bmatrix}$$
		Since we changed the data, we also need to change $f$ so that it takes an element of $\mathbb{R}^2$ instead of an element of $\mathbb{R}$. In this case, we define $g$ such that:
				$$g : \mathbb{R}^2 \times \mathbb{R}^3$$
				$$\vv, \beta \mapsto \beta_0 + \beta_1 \vv_1 + \beta_2 \vv_2$$
		Then, $RP' = (n=2, k, N, g, \beta, X', Y)$ is a regression problem, as we have:
		\begin{align*}
				y &= f(x_i, \beta) + \epsilon_i\\
				  &= \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i1}^2 + \epsilon_i\\
				  &= \beta_0 + \beta_1 x'_{i1} + \beta_2 x'_{i2} + \epsilon_i\\
				  &= g(x'_i, \beta) + \epsilon
		\end{align*}
		Furthermore, $RP'$ is linear, which we can see by looking at the definition of $g$. Finally, as $RP$ and $RP'$ share the same solution $\beta$, we have that $RP'$ is trivially a reduction of $RP$.
\end{example}

\begin{remark}
		Using the same principle than in the last example, polynoms can be linearized.
\end{remark}

\end{document}
